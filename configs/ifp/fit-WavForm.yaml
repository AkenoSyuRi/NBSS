fit:
  seed_everything: null
  trainer:
    logger: true
    checkpoint_callback: true
    callbacks: null
    default_root_dir: null
    gradient_clip_val: 5
    gradient_clip_algorithm: norm
    process_position: 0
    num_nodes: 1
    num_processes: 1
    devices: null
    gpus: null
    auto_select_gpus: false
    tpu_cores: null
    ipus: null
    log_gpu_memory: null
    progress_bar_refresh_rate: null
    overfit_batches: 0.0
    track_grad_norm: -1
    check_val_every_n_epoch: 1
    fast_dev_run: false
    accumulate_grad_batches: 2
    max_epochs: null
    min_epochs: null
    max_steps: null
    min_steps: null
    # max_time="2:0:0:0": stop training for two days passed on single A100, about 500 epochs
    max_time: null
    limit_train_batches: 1.0
    limit_val_batches: 1.0
    limit_test_batches: 1.0
    limit_predict_batches: 1.0
    val_check_interval: 1.0
    flush_logs_every_n_steps: 100
    log_every_n_steps: 50
    accelerator: null
    sync_batchnorm: false
    precision: 32
    weights_summary: top
    weights_save_path: null
    num_sanity_val_steps: 2
    resume_from_checkpoint: null
    profiler: null
    benchmark: false
    deterministic: false
    reload_dataloaders_every_n_epochs: 0
    reload_dataloaders_every_epoch: false
    auto_lr_find: false
    replace_sampler_ddp: true
    terminate_on_nan: false
    auto_scale_batch_size: false
    prepare_data_per_node: null
    # plugins:
    #   - class_path: pytorch_lightning.plugins.DDPPlugin
    #     init_args:
    #       find_unused_parameters: false
    amp_backend: native
    amp_level: O2
    distributed_backend: null
    move_metrics_to_cpu: false
    multiple_trainloader_mode: max_size_cycle
    stochastic_weight_avg: false
  model:
    activation: ''
    target_type: WavForm
    band_num: 1
    ref_channel: 0
    channels:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    ft_len: 512
    ft_overlap: 256
    learning_rate: 0.001
    hidden_size:
    - 256
    - 128
    layer_norm: false
    optimizer_kwargs: {}
    lr_scheduler: ReduceLROnPlateau
    lr_scheduler_kwargs:
      mode: min
      factor: 0.5
      patience: 10
      min_lr: 0.0001
    exp_name: exp
  data:
    clean_speech_dataset: wsj0-mix
    clean_speech_dir: /dev/shm/quancs/
    rir_dir: /dev/shm/quancs/rir_cfg_3
    speech_overlap_ratio:
    - 0.1
    - 1.0
    speech_scale:
    - -5.0
    - 5.0
    batch_size:
    - 15
    - 15
    speaker_num: 2
    audio_time_len:
    - headtail 4
    - headtail 4
    - headtail 4
    num_workers: 15
    test_set: test
    shuffle_train_rir: true
    seeds:
      train: null
      val: 2
      test: 3
  early_stopping:
    monitor: val/neg_si_sdr
    min_delta: 0.01
    patience: 30
    verbose: false
    mode: min
    strict: true
    check_finite: true
    stopping_threshold: null
    divergence_threshold: null
    check_on_train_epoch_end: null
  model_checkpoint:
    dirpath: null
    filename: epoch{epoch}_neg_si_sdr{val/neg_si_sdr:.4f}
    monitor: val/neg_si_sdr
    verbose: false
    save_last: true
    save_top_k: 5
    save_weights_only: false
    mode: min
    auto_insert_metric_name: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: 1
    save_on_train_epoch_end: null
    every_n_val_epochs: null
  learning_rate_monitor:
    logging_interval: epoch
    log_momentum: false
